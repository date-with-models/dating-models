# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1el-JtfuPFK2WVnyblJYjh91scMcpyj_1
"""

from google.colab import files
uploaded = files.upload()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms

import torch
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Load the CIFAR-10 dataset using torchvision.datasets
cifar10_train = datasets.CIFAR10(root='/content/data', train=True, download=True)
cifar10_test = datasets.CIFAR10(root='/content/data', train=False, download=True)

# Define transformations (optional)
transform = transforms.ToTensor()

# Create data loaders
trainloader = DataLoader(cifar10_train, batch_size=64, shuffle=True)
testloader = DataLoader(cifar10_test, batch_size=64, shuffle=False)

!pip install scipy

from scipy.io import loadmat
import numpy as np
import torch

# Mount Google Drive (if you stored the dataset there)
from google.colab import drive
drive.mount('/content/drive')

# Load the data
train_data = loadmat('/content/drive/MyDrive/train_32x32.mat') # replace with your path
test_data = loadmat('/content/drive/MyDrive/test_32x32.mat') # replace with your path

# Extract data and labels (adjust if necessary based on the .mat file structure)
X_train = torch.from_numpy(train_data['X']).permute(3, 2, 0, 1).float()
y_train = torch.from_numpy(train_data['y']).long()

X_test = torch.from_numpy(test_data['X']).permute(3, 2, 0, 1).float()
y_test = torch.from_numpy(test_data['y']).long()

# Example of printing shapes to verify data
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

import torch
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Load the CIFAR-10 dataset using torchvision.datasets
cifar10_train = datasets.CIFAR10(root='/content/data', train=True, download=True)
cifar10_test = datasets.CIFAR10(root='/content/data', train=False, download=True)

# Preprocess and save the data with correct dimensions
torch.save({'data': torch.from_numpy(cifar10_train.data).permute(0, 3, 1, 2),
            'label': torch.tensor(cifar10_train.targets)},
           '/content/data/cifar10-train.pt')
torch.save({'data': torch.from_numpy(cifar10_test.data).permute(0, 3, 1, 2),
            'label': torch.tensor(cifar10_test.targets)},
           '/content/data/cifar10-test.pt')

from ResidualDrop import add_residual_drop
import torch.nn as nn

# Создаем модель, добавляем слой ResidualDrop
class SimpleResNet(nn.Module):
    def __init__(self):
        super(SimpleResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        add_residual_drop(self.layer1, death_rate=0.2, n_channels=128)
        self.fc = nn.Linear(128, 10)

    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.layer1(out)
        out = nn.functional.adaptive_avg_pool2d(out, (1, 1))
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out

# Инициализация модели
model = SimpleResNet()
model = model.to('cuda' if torch.cuda.is_available() else 'cpu')

import os
path = '/content/data'


train_data = torch.load(os.path.join(path, f'cifar10-train.pt'))
test_data_dict = torch.load(os.path.join(path, f'cifar10-test.pt')) # changed variable name

data = torch.tensor(train_data['data']).float()
label = torch.tensor(train_data['label'])

test_data = torch.tensor(test_data_dict['data']).float() # access data from dict
test_label = torch.tensor(test_data_dict['label']) # access label from dict

data = torch.cat((data, test_data), 0)
label = torch.cat((label, test_label), 0)

dataset = torch.utils.data.TensorDataset(data, label)
print(f"train_data type: {type(train_data)}")  # Выведет тип данных

# Если train_data — это словарь, проверим его ключи и типы элементов

print(f"Keys in train_data: {train_data.keys()}")
print(f"train_data['data'] type: {type(train_data['data'])}")
print(f"train_data['label'] type: {type(train_data['label'])}")

import torch

train_data = torch.load('/content/data/cifar10-train.pt')
test_data = torch.load('/content/data/cifar10-test.pt')

print("Train data shape:", train_data['data'].shape)
print("Test data shape:", test_data['data'].shape)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms

# Установка устройства
#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Гиперпараметры
max_epochs = 500
batch_size = 128
death_rate = 0
dataset = 'cifar10'  # или 'svhn' в зависимости от выбранного датасета
# Ensure data_root points to the correct location where the CIFAR-10 dataset is stored.
# If the dataset is not present, it will be downloaded to this location
data_root = '/content/data'

# Загрузка данных
if dataset == 'svhn':
    from svhn_dataset import get_data
elif dataset == 'cifar10':
    from cifar_dataset import get_data

all_data, all_labels = get_data(dataset, data_root, True)
trainloader = DataLoader(all_data, batch_size=batch_size, shuffle=True)
valloader = DataLoader(all_data, batch_size=batch_size)
testloader = DataLoader(all_data, batch_size=batch_size)

# Нормализация данных
mean, std = trainloader.dataset.mean, trainloader.dataset.std
#print(type(trainloader.dataset.mean))
valloader.dataset.transform = transforms.Normalize(mean, std)
testloader.dataset.transform = transforms.Normalize(mean, std)

# Оптимизация и параметры
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)

# Тренировка модели
def train(model, trainloader, optimizer, criterion, epoch):
    model.train()
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(trainloader, 0):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')

# Валидация модели
def validate(model, valloader, criterion):
    model.eval()
    correct = 0
    total = 0
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in valloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Validation Accuracy: {100 * correct / total}%, Loss: {val_loss / len(valloader)}')

# Основной цикл обучения и валидации
for epoch in range(max_epochs):
    train(model, trainloader, optimizer, criterion, epoch)
    validate(model, valloader, criterion)

# Тестирование модели
def test(model, testloader, criterion):
    model.eval()
    correct = 0
    total = 0
    test_loss = 0.0
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            test_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Test Accuracy: {100 * correct / total}%, Loss: {test_loss / len(testloader)}')
